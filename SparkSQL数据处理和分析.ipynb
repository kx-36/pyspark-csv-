{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于文件数量较小，且为了代码的美观性，采用每一次运行需要重新修改路径的方式\n",
    "path=\"data\\d1_m8\"\n",
    "path2=\"data_solve\\d1_m8\"\n",
    "path3=\"data_max_t\\d1_m8\"\n",
    "file=path+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.conf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.master().getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = pyspark.SQLContext(sc)\n",
    "df1 = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(file)\n",
    "#df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据子集选择\n",
    "df2 = df1.drop(' Quality Index').drop(' NTIS Model Version')\n",
    "#df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#列名重命名\n",
    "df3=df2.withColumnRenamed(\" Local Time\", \"Local Time\").withColumnRenamed(\" Day Type ID\",\"Day Type ID\").\\\n",
    "    withColumnRenamed(\" Total Carriageway Flow\", \"Total CF\").\\\n",
    "    withColumnRenamed(\" Total Flow vehicles less than 5.2m\",\"Total Fv<5.2m\").\\\n",
    "    withColumnRenamed(\" Total Flow vehicles 5.21m - 6.6m\", \"Total Fv[5.21m - 6.6m]\").\\\n",
    "    withColumnRenamed(\" Total Flow vehicles 6.61m - 11.6m\",\"Total Fv[6.61m - 11.6m]\").\\\n",
    "    withColumnRenamed(\" Total Flow vehicles above 11.6m\", \"Total Fv> 11.6m\").\\\n",
    "    withColumnRenamed(\" Speed Value\",\"Speed Value\")\n",
    "#df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Local Date                  0\n",
       "Local Time                  0\n",
       "Day Type ID                 0\n",
       "Total CF                    7\n",
       "Total Fv<5.2m               7\n",
       "Total Fv[5.21m - 6.6m]      7\n",
       "Total Fv[6.61m - 11.6m]     7\n",
       "Total Fv> 11.6m             7\n",
       "Speed Value                14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "forward = Window.partitionBy('Local Date').orderBy('Local Time').rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.withColumn('Speed_forward', last('Speed Value', ignorenulls=True).over(forward))#将空值进行填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clear=df3.dropna(subset='Total CF')#删除存在空值的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Local Date                 0\n",
       "Local Time                 0\n",
       "Day Type ID                0\n",
       "Total CF                   0\n",
       "Total Fv<5.2m              0\n",
       "Total Fv[5.21m - 6.6m]     0\n",
       "Total Fv[6.61m - 11.6m]    0\n",
       "Total Fv> 11.6m            0\n",
       "Speed Value                7\n",
       "Speed_forward              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clear.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每一天的平均值车速，并将值取出\n",
    "ave_speed = df_clear.groupBy('Local Date').agg({'Speed_forward': 'mean'})\n",
    "#ave_speed.orderBy(\"Local Date\").show()\n",
    "ave_speed1=ave_speed.withColumn('Local Date', regexp_replace('Local Date','/','-'))\n",
    "ave_speed1 = ave_speed1.withColumn('date', to_date(ave_speed1['Local Date']))\n",
    "ave_speed1 = ave_speed1.withColumn(\"week\",dayofweek('date'))\n",
    "day_ave_spe = ave_speed1.orderBy(\"date\").select(F.collect_list('avg(Speed_forward)')).first()[0]\n",
    "#ave_speed1.select(\"date\",\"week\",\"avg(Speed_forward)\").orderBy('date').toPandas().to_csv(path2+\"Speed.csv\") #导出csv文件\n",
    "#ave_speed1.select(\"date\",\"week\",\"avg(Speed_forward)\").orderBy('date').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每天的出行量，并将值取出\n",
    "ave_cf = df_clear.groupBy('Local Date').agg({'Total CF': 'sum'})\n",
    "ave_cf1=ave_cf.withColumn('Local Date', regexp_replace('Local Date','/','-'))\n",
    "ave_cf1 = ave_cf1.withColumn('date', to_date(ave_cf1['Local Date']))\n",
    "ave_cf1 = ave_cf1.withColumn(\"week\",dayofweek('date'))\n",
    "day_ave_cf = ave_cf1.orderBy('date').select(F.collect_list('sum(Total CF)')).first()[0]\n",
    "#ave_cf1.orderBy('date').show()\n",
    "#ave_cf1.select(\"date\",\"week\",\"sum(Total CF)\").orderBy('date').toPandas().to_csv(path2+\"TotalCF.csv\") #导出csv文件\n",
    "#ave_cf1.select(\"date\",\"week\",\"sum(Total CF)\").orderBy('date').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_hour_PHD(df_day,need_list):\n",
    "    #df_day.show()\n",
    "    df_day1 = (df_day.withColumn('Local Time', F.col('Local Time').cast(TimestampType())))\n",
    "    df_day1=df_day1.withColumn(\"hour\", hour(to_timestamp(\"Local Time\",\"HH:mm:ss\")))\\\n",
    "                .withColumn(\"minute\",minute(to_timestamp(\"Local Time\",\"HH:mm:ss\")))\n",
    "    df_hour_speed=df_day1.groupBy('hour').agg({'Speed Value': 'mean'}).orderBy(\"hour\")\n",
    "    #df_hour_speed.show()\n",
    "    day_speed_data = df_hour_speed.select(F.collect_list('avg(Speed Value)')).first()[0]\n",
    "    #day_speed_data#按时间排序的小时平均车速\n",
    "    #获取高峰小时\n",
    "    df_hour_cf=df_day1.groupBy('hour').agg({'Total CF': 'sum'})\n",
    "    #df_hour_cf.orderBy(\"hour\").show()\n",
    "    df_hour_cf=df_hour_cf.withColumnRenamed(\"sum(Total CF)\", \"sum_Total_CF\")\n",
    "    df_sql_cf=df_hour_cf.createOrReplaceTempView(\"carflow\")\n",
    "    \"\"\" spark.sql(\"select * from carflow\\\n",
    "                where sum_Total_CF=\\\n",
    "                (select max(sum_Total_CF) from carflow)\").show() \"\"\"\n",
    "    m=spark.sql(\"select hour from carflow where sum_Total_CF=(select max(sum_Total_CF) from carflow)\")\n",
    "    max_t=m.select(F.collect_list('hour')).first()[0][0]\n",
    "    print(\"高峰小时为：\",max_t)\n",
    "    need_list.append(max_t)\n",
    "    #求高峰小时系数\n",
    "    df_high_cf=df_day1.filter(df_day1['hour'] == max_t)\n",
    "    max_cf15=df_high_cf.agg({'Total CF': 'max'}).select(F.collect_list('max(Total CF)')).first()[0][0]\n",
    "    max_hour=df_hour_cf.agg({'sum_Total_CF': 'max'}).select(F.collect_list('max(sum_Total_CF)')).first()[0][0]\n",
    "    PHF15=max_hour/(4*max_cf15)\n",
    "    print(\"15分钟高峰小时系数为：{:.2f}\".format(PHF15))\n",
    "    need_list.append(PHF15)\n",
    "    \"\"\"拥堵指数的计算\n",
    "    df_freeflow=df_day1.groupBy('hour').agg({'Total CF': 'sum'}).orderBy(\"hour\")\n",
    "    df_freeflow=df_freeflow.withColumnRenamed(\"sum(Total CF)\", \"sum_Total_CF\")\n",
    "    #df_freeflow.show()\n",
    "    df_sql_freeflow=df_freeflow.createOrReplaceTempView(\"carflow1\")\n",
    "    freeflow_hour=spark.sql(\"select hour from carflow1 where sum_Total_CF=(select min(sum_Total_CF) from carflow1)\").select(F.collect_list('hour')).first()[0][0]\n",
    "    print(\"自由流的时间：\",freeflow_hour)\n",
    "    df_free_min=df_day1.filter(df_day1['hour'] == freeflow_hour)\n",
    "    min_speed_value=df_free_min.agg({'Speed Value': 'mean'}).select(F.collect_list('avg(Speed Value)')).first()[0][0]\n",
    "    print(\"自由流的速度为：\",int(min_speed_value)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_csv():\n",
    "        path_csv = path3+\"max.csv\"\n",
    "        with open(path_csv,'w',newline='') as f:\n",
    "                csv_write = csv.writer(f)\n",
    "                csv_head = [\"day\",\"max_t\",\"PHD15\"]\n",
    "                csv_write.writerow(csv_head)\n",
    "def write_csv(data_row):\n",
    "        path_csv = path3+\"max.csv\"\n",
    "        with open(path_csv,'a',newline='') as f:\n",
    "                csv_write = csv.writer(f)\n",
    "                csv_write.writerow(data_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/8/1\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.96\n",
      "2019/8/2\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.96\n",
      "2019/8/3\n",
      "高峰小时为： 9\n",
      "15分钟高峰小时系数为：0.90\n",
      "2019/8/4\n",
      "高峰小时为： 10\n",
      "15分钟高峰小时系数为：0.97\n",
      "2019/8/5\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.90\n",
      "2019/8/6\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.93\n",
      "2019/8/7\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.94\n",
      "2019/8/8\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.92\n",
      "2019/8/9\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.96\n",
      "2019/8/10\n",
      "高峰小时为： 10\n",
      "15分钟高峰小时系数为：0.96\n",
      "2019/8/11\n",
      "高峰小时为： 10\n",
      "15分钟高峰小时系数为：0.95\n",
      "2019/8/12\n",
      "高峰小时为： 6\n",
      "15分钟高峰小时系数为：0.89\n",
      "2019/8/13\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.93\n",
      "2019/8/14\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.95\n",
      "2019/8/15\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.97\n",
      "2019/8/16\n",
      "高峰小时为： 8\n",
      "15分钟高峰小时系数为：0.97\n",
      "2019/8/17\n",
      "高峰小时为： 9\n",
      "15分钟高峰小时系数为：0.95\n",
      "2019/8/18\n",
      "高峰小时为： 16\n",
      "15分钟高峰小时系数为：0.98\n",
      "2019/8/19\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.95\n",
      "2019/8/20\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.96\n",
      "2019/8/21\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.94\n",
      "2019/8/22\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.98\n",
      "2019/8/23\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.96\n",
      "2019/8/24\n",
      "高峰小时为： 10\n",
      "15分钟高峰小时系数为：0.89\n",
      "2019/8/25\n",
      "高峰小时为： 10\n",
      "15分钟高峰小时系数为：0.93\n",
      "2019/8/26\n",
      "高峰小时为： 17\n",
      "15分钟高峰小时系数为：0.93\n",
      "2019/8/27\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.89\n",
      "2019/8/28\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.92\n",
      "2019/8/29\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.92\n",
      "2019/8/30\n",
      "高峰小时为： 7\n",
      "15分钟高峰小时系数为：0.96\n",
      "2019/8/31\n",
      "高峰小时为： 10\n",
      "15分钟高峰小时系数为：0.96\n"
     ]
    }
   ],
   "source": [
    "create_csv()\n",
    "for i in range (1,32) :\n",
    "    day=\"2019/8/\"+str(i)\n",
    "    need_list=list()\n",
    "    need_list.append(day)\n",
    "    df_day = df_clear.filter(df_clear['Local Date'] == day)#条件过滤\n",
    "    print(day)\n",
    "    max_hour_PHD(df_day,need_list)\n",
    "    write_csv(need_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('python36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96fb195d15fc51374795f80d223d349c0ca94a19ec743b0c6c768538ce377da5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
